{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7615428,"sourceType":"datasetVersion","datasetId":4434986}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Deepfake Face Classification with ResNeXt101 and PyTorch\n\n# Block 1: Install & import necessary packages\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import models, transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport cv2\nimport numpy as np\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:10:35.581016Z","iopub.execute_input":"2025-05-12T09:10:35.581733Z","iopub.status.idle":"2025-05-12T09:10:35.585774Z","shell.execute_reply.started":"2025-05-12T09:10:35.581708Z","shell.execute_reply":"2025-05-12T09:10:35.584965Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:10:46.461532Z","iopub.execute_input":"2025-05-12T09:10:46.462372Z","iopub.status.idle":"2025-05-12T09:10:46.466285Z","shell.execute_reply.started":"2025-05-12T09:10:46.462347Z","shell.execute_reply":"2025-05-12T09:10:46.465513Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Block 2: Face-aware frame extraction function\nface_cascade = cv2.CascadeClassifier(\n    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n)\n\ndef extract_faces_from_video(\n    video_path,\n    frame_count=10,\n    output_size=(128,128),\n    face_cascade=face_cascade\n):\n    cap = cv2.VideoCapture(video_path)\n    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    step = max(total // frame_count, 1)\n\n    faces = []\n    for i in range(frame_count):\n        cap.set(cv2.CAP_PROP_POS_FRAMES, i * step)\n        ret, frame = cap.read()\n        if not ret:\n            break\n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        dets = face_cascade.detectMultiScale(\n            gray, scaleFactor=1.1, minNeighbors=5\n        )\n        if len(dets) > 0:\n            x, y, w, h = max(dets, key=lambda r: r[2]*r[3])\n            face = frame[y:y+h, x:x+w]\n            face = cv2.resize(face, output_size)\n            faces.append(face)\n    cap.release()\n    return faces  # list of 10 cropped face images or fewer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:11:02.540989Z","iopub.execute_input":"2025-05-12T09:11:02.541542Z","iopub.status.idle":"2025-05-12T09:11:02.572847Z","shell.execute_reply.started":"2025-05-12T09:11:02.541508Z","shell.execute_reply":"2025-05-12T09:11:02.572331Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Block 3: Prepare face dataset (extract, label, split)\nREAL_VID_DIR = \"/kaggle/input/faceforensics/FF++/real\"\nFAKE_VID_DIR = \"/kaggle/input/faceforensics/FF++/fake\"\nFRAME_COUNT = 10\nTMP_DIR = \"./faces_tmp\"\nos.makedirs(TMP_DIR, exist_ok=True)\n\nfilepaths, labels = [], []\n\n# extract faces and save images\nfor label, vid_dir in [(0, REAL_VID_DIR), (1, FAKE_VID_DIR)]:\n    for vf in tqdm(os.listdir(vid_dir)[:200], desc=f\"Processing {'Real' if label==0 else 'Fake'}\"):\n        vid_path = os.path.join(vid_dir, vf)\n        faces = extract_faces_from_video(vid_path, FRAME_COUNT)\n        if len(faces) == FRAME_COUNT:\n            for idx, face in enumerate(faces):\n                out_path = os.path.join(TMP_DIR, f\"{label}_{vf}_{idx}.jpg\")\n                cv2.imwrite(out_path, face)\n                filepaths.append(out_path)\n                labels.append(label)\n\n# train/val split\nX_train, X_val, y_train, y_val = train_test_split(\n    filepaths, labels, test_size=0.2, stratify=labels, random_state=42\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:11:22.451128Z","iopub.execute_input":"2025-05-12T09:11:22.451374Z","iopub.status.idle":"2025-05-12T09:59:17.797025Z","shell.execute_reply.started":"2025-05-12T09:11:22.451359Z","shell.execute_reply":"2025-05-12T09:59:17.796446Z"}},"outputs":[{"name":"stderr","text":"Processing Real: 100%|██████████| 200/200 [24:06<00:00,  7.23s/it]\nProcessing Fake: 100%|██████████| 200/200 [23:48<00:00,  7.14s/it]\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Block 4: Dataset and DataLoader\nclass FaceFrameDataset(Dataset):\n    def __init__(self, paths, labels, transform=None):\n        self.paths = paths\n        self.labels = labels\n        self.transform = transform\n    def __len__(self): return len(self.paths)\n    def __getitem__(self, idx):\n        img = cv2.imread(self.paths[idx])\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        if self.transform:\n            img = self.transform(img)\n        return img, self.labels[idx]\n\ntransform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((128,128)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n])\n\ntrain_ds = FaceFrameDataset(X_train, y_train, transform)\nval_ds   = FaceFrameDataset(X_val,   y_val,   transform)\n\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\nval_loader   = DataLoader(val_ds,   batch_size=32, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:59:23.110015Z","iopub.execute_input":"2025-05-12T09:59:23.110492Z","iopub.status.idle":"2025-05-12T09:59:23.116571Z","shell.execute_reply.started":"2025-05-12T09:59:23.110461Z","shell.execute_reply":"2025-05-12T09:59:23.115827Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Block 5: Load ResNeXt101 classifier\nmodel = models.resnext101_32x8d(pretrained=True)\nfor param in model.parameters(): param.requires_grad = False\nmodel.fc = nn.Sequential(\n    nn.Linear(model.fc.in_features, 512),\n    nn.ReLU(),\n    nn.Dropout(0.3),\n    nn.Linear(512, 2)\n)\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:59:26.220750Z","iopub.execute_input":"2025-05-12T09:59:26.221203Z","iopub.status.idle":"2025-05-12T09:59:28.031555Z","shell.execute_reply.started":"2025-05-12T09:59:26.221181Z","shell.execute_reply":"2025-05-12T09:59:28.030989Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNeXt101_32X8D_Weights.IMAGENET1K_V1`. You can also use `weights=ResNeXt101_32X8D_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"\n# Block 6: Training loop\ndef train(model, train_loader, val_loader, epochs=5):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.fc.parameters(), lr=1e-4)\n    for epoch in range(epochs):\n        model.train()\n        total_loss, total_correct = 0, 0\n        for imgs, lbls in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n            imgs, lbls = imgs.to(device), lbls.to(device)\n            optimizer.zero_grad()\n            outputs = model(imgs)\n            loss = criterion(outputs, lbls)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * imgs.size(0)\n            total_correct += (outputs.argmax(1)==lbls).sum().item()\n        print(f\"Train Loss: {total_loss/len(train_loader.dataset):.4f}\",\n              f\"Acc: {total_correct/len(train_loader.dataset):.4f}\")\n\n        model.eval()\n        val_loss, val_correct = 0, 0\n        with torch.no_grad():\n            for imgs, lbls in val_loader:\n                imgs, lbls = imgs.to(device), lbls.to(device)\n                out = model(imgs)\n                val_loss += criterion(out, lbls).item() * imgs.size(0)\n                val_correct += (out.argmax(1)==lbls).sum().item()\n        print(f\"Val Loss: {val_loss/len(val_loader.dataset):.4f}\",\n              f\"Val Acc: {val_correct/len(val_loader.dataset):.4f}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:59:32.220791Z","iopub.execute_input":"2025-05-12T09:59:32.221361Z","iopub.status.idle":"2025-05-12T09:59:32.228016Z","shell.execute_reply.started":"2025-05-12T09:59:32.221339Z","shell.execute_reply":"2025-05-12T09:59:32.227292Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Block 7: Run training\ntrain(model, train_loader, val_loader, epochs=40)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T10:01:41.901576Z","iopub.execute_input":"2025-05-12T10:01:41.901902Z","iopub.status.idle":"2025-05-12T10:08:10.317337Z","shell.execute_reply.started":"2025-05-12T10:01:41.901879Z","shell.execute_reply":"2025-05-12T10:08:10.316712Z"}},"outputs":[{"name":"stderr","text":"Epoch 1: 100%|██████████| 76/76 [00:07<00:00,  9.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3081 Acc: 0.8697\nVal Loss: 0.3699 Val Acc: 0.8273\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 76/76 [00:07<00:00,  9.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3174 Acc: 0.8544\nVal Loss: 0.3466 Val Acc: 0.8602\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 76/76 [00:07<00:00,  9.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2995 Acc: 0.8680\nVal Loss: 0.3542 Val Acc: 0.8487\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 76/76 [00:07<00:00,  9.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3050 Acc: 0.8643\nVal Loss: 0.3740 Val Acc: 0.8289\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|██████████| 76/76 [00:07<00:00,  9.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2787 Acc: 0.8816\nVal Loss: 0.3497 Val Acc: 0.8454\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6: 100%|██████████| 76/76 [00:07<00:00,  9.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2755 Acc: 0.8836\nVal Loss: 0.3341 Val Acc: 0.8487\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7: 100%|██████████| 76/76 [00:07<00:00,  9.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2667 Acc: 0.8816\nVal Loss: 0.3411 Val Acc: 0.8569\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8: 100%|██████████| 76/76 [00:07<00:00,  9.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2523 Acc: 0.8939\nVal Loss: 0.3340 Val Acc: 0.8635\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9: 100%|██████████| 76/76 [00:07<00:00,  9.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2473 Acc: 0.8984\nVal Loss: 0.3481 Val Acc: 0.8454\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10: 100%|██████████| 76/76 [00:07<00:00,  9.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2410 Acc: 0.8956\nVal Loss: 0.3352 Val Acc: 0.8553\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11: 100%|██████████| 76/76 [00:07<00:00,  9.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2404 Acc: 0.8993\nVal Loss: 0.3232 Val Acc: 0.8569\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12: 100%|██████████| 76/76 [00:07<00:00,  9.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2394 Acc: 0.9046\nVal Loss: 0.3147 Val Acc: 0.8684\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13: 100%|██████████| 76/76 [00:07<00:00,  9.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2361 Acc: 0.9054\nVal Loss: 0.3168 Val Acc: 0.8783\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14: 100%|██████████| 76/76 [00:07<00:00,  9.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2166 Acc: 0.9083\nVal Loss: 0.3084 Val Acc: 0.8750\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15: 100%|██████████| 76/76 [00:07<00:00,  9.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2131 Acc: 0.9141\nVal Loss: 0.3833 Val Acc: 0.8520\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16: 100%|██████████| 76/76 [00:07<00:00,  9.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1901 Acc: 0.9268\nVal Loss: 0.3061 Val Acc: 0.8832\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17: 100%|██████████| 76/76 [00:07<00:00,  9.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1967 Acc: 0.9211\nVal Loss: 0.3289 Val Acc: 0.8750\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18: 100%|██████████| 76/76 [00:07<00:00,  9.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1913 Acc: 0.9231\nVal Loss: 0.3111 Val Acc: 0.8816\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19: 100%|██████████| 76/76 [00:07<00:00,  9.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1911 Acc: 0.9194\nVal Loss: 0.3059 Val Acc: 0.8816\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20: 100%|██████████| 76/76 [00:07<00:00,  9.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1860 Acc: 0.9293\nVal Loss: 0.3403 Val Acc: 0.8602\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21: 100%|██████████| 76/76 [00:07<00:00,  9.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1747 Acc: 0.9338\nVal Loss: 0.3140 Val Acc: 0.8799\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22: 100%|██████████| 76/76 [00:07<00:00,  9.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1611 Acc: 0.9367\nVal Loss: 0.2978 Val Acc: 0.8849\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23: 100%|██████████| 76/76 [00:07<00:00,  9.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1600 Acc: 0.9416\nVal Loss: 0.3201 Val Acc: 0.8783\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24: 100%|██████████| 76/76 [00:07<00:00,  9.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1624 Acc: 0.9396\nVal Loss: 0.3181 Val Acc: 0.8766\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25: 100%|██████████| 76/76 [00:07<00:00,  9.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1562 Acc: 0.9465\nVal Loss: 0.3286 Val Acc: 0.8684\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26: 100%|██████████| 76/76 [00:07<00:00,  9.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1653 Acc: 0.9350\nVal Loss: 0.3285 Val Acc: 0.8766\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27: 100%|██████████| 76/76 [00:07<00:00,  9.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1614 Acc: 0.9379\nVal Loss: 0.3206 Val Acc: 0.8783\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28: 100%|██████████| 76/76 [00:07<00:00,  9.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1536 Acc: 0.9412\nVal Loss: 0.3105 Val Acc: 0.8717\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29: 100%|██████████| 76/76 [00:07<00:00,  9.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1405 Acc: 0.9433\nVal Loss: 0.3268 Val Acc: 0.8717\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30: 100%|██████████| 76/76 [00:07<00:00,  9.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1541 Acc: 0.9412\nVal Loss: 0.3100 Val Acc: 0.8783\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31: 100%|██████████| 76/76 [00:07<00:00,  9.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1335 Acc: 0.9527\nVal Loss: 0.3014 Val Acc: 0.8849\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32: 100%|██████████| 76/76 [00:07<00:00,  9.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1321 Acc: 0.9482\nVal Loss: 0.2967 Val Acc: 0.9013\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33: 100%|██████████| 76/76 [00:07<00:00,  9.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1335 Acc: 0.9486\nVal Loss: 0.3029 Val Acc: 0.8783\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34: 100%|██████████| 76/76 [00:07<00:00,  9.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1370 Acc: 0.9494\nVal Loss: 0.3053 Val Acc: 0.8898\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35: 100%|██████████| 76/76 [00:07<00:00,  9.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1238 Acc: 0.9552\nVal Loss: 0.3090 Val Acc: 0.8898\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36: 100%|██████████| 76/76 [00:07<00:00,  9.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1379 Acc: 0.9445\nVal Loss: 0.3059 Val Acc: 0.8849\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37: 100%|██████████| 76/76 [00:07<00:00,  9.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1249 Acc: 0.9531\nVal Loss: 0.3293 Val Acc: 0.8832\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38: 100%|██████████| 76/76 [00:07<00:00,  9.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1186 Acc: 0.9552\nVal Loss: 0.3143 Val Acc: 0.8799\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39: 100%|██████████| 76/76 [00:07<00:00,  9.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1174 Acc: 0.9544\nVal Loss: 0.3058 Val Acc: 0.8832\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40: 100%|██████████| 76/76 [00:07<00:00,  9.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1287 Acc: 0.9461\nVal Loss: 0.3089 Val Acc: 0.8816\n\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# Block 8: Save model\ntorch.save(model.state_dict(), \"resnext101_deepfake_faces.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T10:12:16.065367Z","iopub.execute_input":"2025-05-12T10:12:16.065927Z","iopub.status.idle":"2025-05-12T10:12:16.560880Z","shell.execute_reply.started":"2025-05-12T10:12:16.065906Z","shell.execute_reply":"2025-05-12T10:12:16.560275Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# === Block: Model Testing & Evaluation ===\n\nimport torch\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 1) Rebuild dataset & loader for “test” splits:\n#    If you already have X_test & y_test lists from your split, skip this and just wrap them,\n#    otherwise you can reuse the FaceFrameDataset class and point it at your saved faces.\ntest_ds = FaceFrameDataset(X_val, y_val, transform)   # or use X_test,y_test if you set aside a test split\ntest_loader = DataLoader(test_ds, batch_size=32, shuffle=False)\n\n# 2) Load the trained weights\nmodel.load_state_dict(torch.load(\"resnext101_deepfake_faces.pth\", map_location=device))\nmodel.eval()\n\n# 3) Collect predictions and true labels\nall_preds, all_labels = [], []\nwith torch.no_grad():\n    for imgs, lbls in test_loader:\n        imgs = imgs.to(device)\n        out = model(imgs)\n        preds = out.argmax(dim=1).cpu().numpy()\n        all_preds.extend(preds)\n        all_labels.extend(lbls.numpy())\n\nall_preds  = np.array(all_preds)\nall_labels = np.array(all_labels)\n\n# 4) Compute metrics\nacc = accuracy_score(all_labels, all_preds)\nprint(f\"Test Accuracy: {acc*100:.2f}%\\n\")\n\nprint(\"Classification Report:\")\nprint(classification_report(all_labels, all_preds, target_names=[\"REAL\",\"FAKE\"]))\n\n# 5) Confusion Matrix\ncm = confusion_matrix(all_labels, all_preds)\nplt.figure(figsize=(6,5))\nsns.heatmap(cm, annot=True, fmt=\"d\", \n            xticklabels=[\"REAL\",\"FAKE\"], \n            yticklabels=[\"REAL\",\"FAKE\"])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T10:12:19.245696Z","iopub.execute_input":"2025-05-12T10:12:19.246346Z","iopub.status.idle":"2025-05-12T10:12:22.393852Z","shell.execute_reply.started":"2025-05-12T10:12:19.246324Z","shell.execute_reply":"2025-05-12T10:12:22.393214Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/1050624176.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"resnext101_deepfake_faces.pth\", map_location=device))\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 88.16%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n        REAL       0.85      0.91      0.88       282\n        FAKE       0.92      0.86      0.89       326\n\n    accuracy                           0.88       608\n   macro avg       0.88      0.88      0.88       608\nweighted avg       0.88      0.88      0.88       608\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 600x500 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"import torch\n\n# 1) Ensure your model and utilities are loaded:\n#    - model: your ResNeXt101 face‐crop classifier (already .to(device))\n#    - extract_faces_from_video(), transform, device from earlier code\n\n# Load trained weights\nmodel.load_state_dict(torch.load(\"resnext101_deepfake_faces.pth\", map_location=device))\n\n# 2) Define the sample video path (replace with your file)\nsample_path = \"/kaggle/input/faceforensics/FF++/fake/01_02__outside_talking_still_laughing__YVGY8LOK.mp4\"\n\n# 3) Run prediction\nlabel, confidence, details = predict_video(\n    sample_path,\n    model,\n    transform,\n    device,\n    frame_count=10,\n    return_details=True\n)\n\n# 4) Print results\nprint(\"=== Video Deepfake Prediction ===\")\nprint(f\"Video File : {sample_path}\")\nprint(f\"Prediction : {label}\")\nprint(f\"Confidence : {confidence:.2f}\")\nprint(\"\\nPer-frame probabilities:\")\nfor idx, (r_prob, f_prob) in enumerate(zip(details[\"real_probs\"], details[\"fake_probs\"])):\n    print(f\" Frame {idx+1:2d} → REAL: {r_prob:.2f}, FAKE: {f_prob:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T10:18:32.860967Z","iopub.execute_input":"2025-05-12T10:18:32.861517Z","iopub.status.idle":"2025-05-12T10:18:40.134599Z","shell.execute_reply.started":"2025-05-12T10:18:32.861497Z","shell.execute_reply":"2025-05-12T10:18:40.133849Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/2820769953.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"resnext101_deepfake_faces.pth\", map_location=device))\n","output_type":"stream"},{"name":"stdout","text":"=== Video Deepfake Prediction ===\nVideo File : /kaggle/input/faceforensics/FF++/fake/01_02__outside_talking_still_laughing__YVGY8LOK.mp4\nPrediction : FAKE\nConfidence : 0.98\n\nPer-frame probabilities:\n Frame  1 → REAL: 0.00, FAKE: 1.00\n Frame  2 → REAL: 0.00, FAKE: 1.00\n Frame  3 → REAL: 0.04, FAKE: 0.96\n Frame  4 → REAL: 0.00, FAKE: 1.00\n Frame  5 → REAL: 0.01, FAKE: 0.99\n Frame  6 → REAL: 0.12, FAKE: 0.88\n Frame  7 → REAL: 0.00, FAKE: 1.00\n Frame  8 → REAL: 0.01, FAKE: 0.99\n Frame  9 → REAL: 0.01, FAKE: 0.99\n Frame 10 → REAL: 0.00, FAKE: 1.00\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"import torch\n\n# 1) Ensure your model and utilities are loaded:\n#    - model: your ResNeXt101 face‐crop classifier (already .to(device))\n#    - extract_faces_from_video(), transform, device from earlier code\n\n# Load trained weights\nmodel.load_state_dict(torch.load(\"resnext101_deepfake_faces.pth\", map_location=device))\n\n# 2) Define the sample video path (replace with your file)\nsample_path = \"/kaggle/input/faceforensics/FF++/real/01__podium_speech_happy.mp4\"\n\n# 3) Run prediction\nlabel, confidence, details = predict_video(\n    sample_path,\n    model,\n    transform,\n    device,\n    frame_count=10,\n    return_details=True\n)\n\n# 4) Print results\nprint(\"=== Video Deepfake Prediction ===\")\nprint(f\"Video File : {sample_path}\")\nprint(f\"Prediction : {label}\")\nprint(f\"Confidence : {confidence:.2f}\")\nprint(\"\\nPer-frame probabilities:\")\nfor idx, (r_prob, f_prob) in enumerate(zip(details[\"real_probs\"], details[\"fake_probs\"])):\n    print(f\" Frame {idx+1:2d} → REAL: {r_prob:.2f}, FAKE: {f_prob:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T10:19:57.261200Z","iopub.execute_input":"2025-05-12T10:19:57.261696Z","iopub.status.idle":"2025-05-12T10:20:02.564837Z","shell.execute_reply.started":"2025-05-12T10:19:57.261672Z","shell.execute_reply":"2025-05-12T10:20:02.564026Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/743551065.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"resnext101_deepfake_faces.pth\", map_location=device))\n","output_type":"stream"},{"name":"stdout","text":"=== Video Deepfake Prediction ===\nVideo File : /kaggle/input/faceforensics/FF++/real/01__podium_speech_happy.mp4\nPrediction : REAL\nConfidence : 0.97\n\nPer-frame probabilities:\n Frame  1 → REAL: 1.00, FAKE: 0.00\n Frame  2 → REAL: 0.80, FAKE: 0.20\n Frame  3 → REAL: 1.00, FAKE: 0.00\n Frame  4 → REAL: 0.97, FAKE: 0.03\n Frame  5 → REAL: 0.98, FAKE: 0.02\n Frame  6 → REAL: 0.99, FAKE: 0.01\n Frame  7 → REAL: 0.99, FAKE: 0.01\n Frame  8 → REAL: 1.00, FAKE: 0.00\n Frame  9 → REAL: 0.99, FAKE: 0.01\n Frame 10 → REAL: 1.00, FAKE: 0.00\n","output_type":"stream"}],"execution_count":25}]}